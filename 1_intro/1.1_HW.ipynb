{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "number_of_pages = 100\n",
    "job_title = [\"'Инженер данных' and 'Дата инженер' and 'Data engineer'\"]\n",
    "for job in job_title:\n",
    "    data=[]\n",
    "    for i in range(number_of_pages):\n",
    "        url = 'https://api.hh.ru/vacancies'\n",
    "        par = {'text': job, 'area':'113','per_page':'10', 'page':i}\n",
    "        r = requests.get(url, params=par)\n",
    "        e=r.json()\n",
    "        data.append(e)\n",
    "        vacancy_details = data[0]['items'][0].keys()\n",
    "        df = pd.DataFrame(columns= list(vacancy_details))\n",
    "        ind = 0\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i]['items'])):\n",
    "                df.loc[ind] = data[i]['items'][j]\n",
    "                ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_refs = list(map(lambda x: 'https://api.hh.ru/vacancies/' + x, df['id']))\n",
    "data1=[]\n",
    "data2=[]\n",
    "for i in all_refs:\n",
    "    r = requests.get(i)\n",
    "    e=r.json()\n",
    "    data1.append(e['key_skills'])\n",
    "    data2.append(e['experience'])\n",
    "    df1 = pd.DataFrame(columns= list(data1[0][0]))\n",
    "    df2 = pd.DataFrame(columns= list(data2[0]))\n",
    "    ind1 = 0\n",
    "    ind2 = 0\n",
    "    for i1 in range(len(data1)):\n",
    "        for j1 in range(len(data1[i1])):\n",
    "            df1.loc[ind1] = data1[i1][j1]\n",
    "            ind1+=1\n",
    "    for i2 in range(len(data2)):\n",
    "        df2.loc[ind2] = data2[i2]\n",
    "        ind2+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(1, \"count\", 1)\n",
    "df2.insert(1, \"count\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.groupby('name').count().sort_values(by='count',ascending=False).reset_index().head(10)\n",
    "df2 = df2.groupby('name').count().sort_values(by='count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для составления отчета был взять ресурс Head Hunter и произведен поиск на 100 страницах \n",
      "1: Количество найденных вакансий: 85 премущественно в IT отрасли с разным размером компании.\n",
      "\n",
      "2: Десять наиболее упоминаемых требований по языкам, технологиям, фреймворкам к соискателю вакансии :\n",
      "          name  count\n",
      "0       Python     55\n",
      "1          SQL     53\n",
      "2          ETL     22\n",
      "3         Java     16\n",
      "4       Hadoop     15\n",
      "5          DWH     15\n",
      "6          Git     13\n",
      "7        Spark     12\n",
      "8  Базы данных     11\n",
      "9   PostgreSQL     10\n",
      "\n",
      "Требования по опыту работы: \n",
      "                  name  id  count\n",
      "0       От 3 до 6 лет  48     48\n",
      "1  От 1 года до 3 лет  34     34\n",
      "2           Нет опыта   2      2\n",
      "3         Более 6 лет   1      1\n",
      "\n",
      "3: Целями моего обучения является получение новых и закрепления полученных ранее знаний и навыков по работе с данными.\n",
      "Python, SQL, Hadoop, Kafka, ETL, Spark, AirfFlow, Искусственный интеллект, ML\n"
     ]
    }
   ],
   "source": [
    "print(f'Для составления отчета был взять ресурс Head Hunter\\\n",
    " и произведен поиск на 100 страницах \\n1: Количество найденных\\\n",
    " вакансий: {len(df.index)} премущественно в IT отрасли с разным\\\n",
    " размером компании.\\n\\n2: Десять наиболее упоминаемых требований\\\n",
    " по языкам, технологиям, фреймворкам к соискателю вакансии :\\n{df1}\\\n",
    "\\n\\nТребования по опыту работы: \\n {df2}\\n\\n\\\n",
    "3: Целями моего обучения является получение новых и закрепления\\\n",
    " полученных ранее знаний и навыков по работе с данными.\\n\\\n",
    "Python, SQL, Hadoop, Kafka, ETL, Spark, AirfFlow, Искусственный интеллект, ML')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
